# -*- coding: utf-8 -*-
"""FRCNNRPN261220.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y9lxb9Iq4YmaYhRdvl2JOZIvG10asFPg
"""

import os
import numpy as np
import torch
from PIL import Image
import sys
import cv2
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt

!pip install Pillow opencv-python sklearn numpy

!git clone https://github.com/nicolaihaeni/MinneApple.git

cd /content/drive/MyDrive/MinneApple

!mkdir dataset1

cd /content/drive/MyDrive/MinneApple/dataset1/train

!mkdir masks





!mkdir models_39_100

cd /content/drive/MyDrive/MinneApple

#firstly 30 epoches
#will try 70 epoches

!python train_rcnn.py --data_path /content/drive/MyDrive/MinneApple/dataset1 --model frcnn --epochs 62 --output-dir /content/drive/MyDrive/MinneApple/models_39_100 --b 2 --lr 0.0001 --resume /content/drive/MyDrive/MinneApple/models_30_100/model_7.pth

cd /content/drive/MyDrive/MinneApple

# Predict for Faster RCNN
!python predict_rcnn.py --data_path /content/drive/MyDrive/MinneApple/dataset1/prediction/ref --output_file /content/drive/MyDrive/MinneApple/dataset1/prediction/res/model_61_result.txt --weight_file /content/drive/MyDrive/MinneApple/models_39_100/model_61.pth --device cpu --frcnn

cd /content/drive/MyDrive/MinneApple/dataset/prediction

!mkdir res

!mkdir ref

cd /content/drive/MyDrive/MinneApple

!python detection_eval.py /content/drive/MyDrive/MinneApple/dataset/prediction /content/drive/MyDrive/MinneApple/evaluation

txts_path = get_file_paths(txts_dir)  ## drawing bounding box in images

# for txt in txts_path:

#     img_name = txt.split('/')[-1].split('.')[0] + '.png'
#     img_path = os.path.join(imgs_dir, img_name)
#     print(img_path)

#     img_label = get_save_file(save_img_label_dir, img_name)

#     img = cv2.imread(img_path)
#     boxs = np.loadtxt(txt)

#     for i, box in enumerate(boxs):

#       cv2.rectangle(img,(int(box[0]),int(box[1])),(int(box[2]),int(box[3])),(0,0,255),1)
#       cv2.putText(img, str(i+1), (int(box[0]),int(box[1])),cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1)

#     cv2.imwrite(img_label, img)


# label = cv2.imread('./train/masks/20150919_174151_image171.png')
# img = cv2.imread('./train/images/20150919_174151_image171.png')

# a = box_and_num(label)

# print(a)

# for i, box in enumerate(boxs):

#   cv2.rectangle(img,(box[0],box[1]),(box[2],box[3]),(0,255,0),1)
#   cv2.putText(img, str(i+1), (box[0],box[1]),cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1)

# cv2_imshow(label)

def vis_detections(im, class_name, dets, thresh):
    """Visual debugging of detections."""
    num = 0
    bboxes = []
    for i in range(dets.shape[0]):
        bbox = tuple(int(np.round(x)) for x in dets[i, :4])
        score = dets[i, -1]
        if score > thresh:
            num = num + 1
            bboxes.append(bbox)
            cv2.rectangle(im, bbox[0:2], bbox[2:4], (0, 204, 0), 2)
            cv2.putText(im, '%s: %.3f' % (class_name, score), (bbox[0], bbox[1] + 15), cv2.FONT_HERSHEY_PLAIN,
                        1.0, (0, 0, 255), thickness=1)
            
    return im, num, bboxes

def py_cpu_nms(dets, thresh):
    """Pure Python NMS baseline."""
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]

    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]  #[::-1]表示降序排序，输出为其对应序号

    keep = []                     #需要保留的bounding box
    while order.size > 0:
        i = order[0]              #取置信度最大的（即第一个）框
        keep.append(i)            #将其作为保留的框
        
        #以下计算置信度最大的框（order[0]）与其它所有的框（order[1:]，即第二到最后一个）框的IOU，以下都是以向量形式表示和计算
        xx1 = np.maximum(x1[i], x1[order[1:]]) #计算xmin的max,即overlap的xmin
        yy1 = np.maximum(y1[i], y1[order[1:]]) #计算ymin的max,即overlap的ymin
        xx2 = np.minimum(x2[i], x2[order[1:]]) #计算xmax的min,即overlap的xmax
        yy2 = np.minimum(y2[i], y2[order[1:]]) #计算ymax的min,即overlap的ymax

        w = np.maximum(0.0, xx2 - xx1 + 1)      #计算overlap的width
        h = np.maximum(0.0, yy2 - yy1 + 1)      #计算overlap的hight
        inter = w * h                           #计算overlap的面积
        ovr = inter / (areas[i] + areas[order[1:]] - inter) #计算并，-inter是因为交集部分加了两次。

        inds = np.where(ovr <= thresh)[0]          #本轮，order仅保留IOU不大于阈值的下标
        order = order[inds + 1]                    #删除IOU大于阈值的框

    return keep

def box_and_num(label):

  n_apple = label.max()

  boxs = []
  for i in range(1, n_apple+1):

    pos = np.where(label[:,:,1] == i)
    pos = np.array([pos[0].tolist(), pos[1].tolist()])

    if np.size(pos[0]) != 0:   # miss some labels, so no pixels

      x_min = pos[1].min()
      x_max = pos[1].max()
      x_c = (x_max - x_min)/2

      y_min = pos[0].min()
      y_max = pos[0].max()
      y_c = (y_max - y_min)/2
      

      boxs.append([x_min,y_min,x_max,y_max])
  
  if len(boxs) == n_apple:

    return boxs, n_apple

  else:

    return [], n_apple

img_dir = '/content/drive/MyDrive/MinneApple/dataset1/prediction/ref/images'
mask_dir = '/content/drive/MyDrive/MinneApple/dataset1/prediction/ref/masks'
result_path = '/content/drive/MyDrive/MinneApple/dataset1/prediction/res/model_61_result.txt'

img_files = os.listdir(img_dir)

imgs_name = []
imgs_score = []
imgs_box = []

# for file in img_files:
  
with open(result_path) as f:

  contents = f.readlines()
  # pos = np.where(contents[:][0:10] == file)
  num = len(contents)

  for i in range(num):

      img_name = contents[i].split(',')[0]
      img_score = float(contents[i].split(',')[5].split('\n')[0])
      xmin = contents[i].split(',')[1]
      ymin = contents[i].split(',')[2]
      xmax = contents[i].split(',')[3]
      ymax = contents[i].split(',')[4]
      img_box = [float(xmin), float(ymin), float(xmax), float(ymax)]

      # img_path = os.path.join(img_dir, img_name)
      imgs_name.append(img_name)
      imgs_score.append(img_score)
      imgs_box.append(img_box)

imgs_name = np.array(imgs_name)
# imgs_name = imgs_name.reshape(imgs_name.shape[0], 1)
imgs_score = np.array(imgs_score)
imgs_score = imgs_score.reshape(imgs_score.shape[0], 1)
imgs_box = np.array(imgs_box)
imgs_box = imgs_box.reshape(imgs_box.shape[0], 4)

box_score = np.concatenate((imgs_box, imgs_score), axis=1)


name_list = np.unique(imgs_name)
each_result = []
gt_est = []

for name in name_list:

    img_id = np.where(imgs_name == name)[0]
    box_each = box_score[img_id,:]
    nhs_id = py_cpu_nms(box_each, 0.4)
    box_filtered = box_each[nhs_id]
    each_result.append(box_filtered) 
    img_path = os.path.join(img_dir, name)
    mask_path = os.path.join(mask_dir, name)
    im = cv2.imread(img_path)
    mask = cv2.imread(mask_path)
    gt = mask.max()
    class_name = 'apple'
    dets = box_filtered
    thresh = 0.9
    im, num, bbox = vis_detections(im, class_name, dets, thresh)
    gt_est.append([gt, num])
    print([gt, num])
    # cv2_imshow(im)

gt_est = np.array(gt_est)
diff = 1 - abs(gt_est[:, 0] - gt_est[:, 1])/gt_est[:, 0]
print(sum(diff)/len(diff))
print(diff)

plt.figure(figsize=(25,10))
plt.plot(range(len(gt_est)), gt_est[:, 0], 'b-', label = "ground truth")
plt.plot(range(len(gt_est)), gt_est[:, 1],'r--', label = "estimation")
plt.grid(True)
plt.legend()
plt.xlabel('APPLE_ID')
plt.ylabel('NUMBER')
plt.show()

img_path = os.path.join(img_dir,"000002.png")
im = cv2.imread(img_path)
class_name = 'apple'
dets = each_result[1]
thresh = 0.5
im, num, bbox = vis_detections(im, class_name, dets, thresh)
label = cv2.imread('/content/drive/MyDrive/MinneApple/dataset1/prediction/ref/masks/000002.png')
gt_boxs = box_and_num(label)[0]
print(len(gt_boxs), num)
for i, box in enumerate(gt_boxs):

  cv2.rectangle(im,(box[0],box[1]),(box[2],box[3]),(255,0,0),1)
  cv2.putText(im, str(i+1), (box[0],box[1]),cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1)

cv2_imshow(im)
print(len(bbox))

#

a = each_result[-4]
print(len(a))
len(py_cpu_nms(a, 0.4))

label = cv2.imread('./train/masks/20150919_174151_image171.png')
img = cv2.imread('./train/images/20150919_174151_image171.png')

a = box_and_num(label)

print(a)

for i, box in enumerate(boxs):

  cv2.rectangle(img,(box[0],box[1]),(box[2],box[3]),(0,255,0),1)
  cv2.putText(img, str(i+1), (box[0],box[1]),cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1)

cv2_imshow(img)